# HR Workday Data Pipeline - AWS Deployment

This package contains everything needed to deploy an HR Workday data pipeline on AWS, loading simulated HR data from S3 into Redshift Serverless using AWS Glue.

## Architecture

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   CSV Files     │────▶│   S3 Bucket     │────▶│   AWS Glue      │
│   (Local)       │     │   (Raw Data)    │     │   (ETL Jobs)    │
└─────────────────┘     └─────────────────┘     └────────┬────────┘
                                                         │
                                                         ▼
                                                ┌─────────────────┐
                                                │   Redshift      │
                                                │   Serverless    │
                                                └─────────────────┘
```

## Data Model

| Table | Description | Records |
|-------|-------------|---------|
| `core_hr_employees` | Non-transactional employee master data | ~10,000 |
| `job_movement_transactions` | Hires, promotions, terminations | ~7,000 |
| `compensation_change_transactions` | Salary, bonus, equity changes | ~20,000 |
| `worker_movement_transactions` | Transfers, relocations, org changes | ~13,000 |

## Prerequisites

1. **AWS CLI** configured with appropriate credentials
2. **jq** (optional, for JSON parsing)
3. **CSV data files** generated by the Python script

### Required AWS Permissions

The deploying user/role needs permissions for:
- S3 (bucket creation, object upload)
- IAM (role creation via CloudFormation)
- Redshift Serverless (namespace, workgroup creation)
- AWS Glue (database, crawler, job creation)
- Secrets Manager (for Redshift credentials)
- CloudFormation (stack deployment)

## Quick Start

### 1. Generate the HR Data (if not already done)

```bash
python workday_hr_data_generator.py
```

### 2. Run the Deployment

```bash
cd aws_deployment
chmod +x deploy.sh scripts/*.sh
./deploy.sh /path/to/csv/files
```

The master deployment script will:
1. Create an S3 bucket for HR data
2. Deploy IAM roles via CloudFormation
3. Upload CSV files to S3
4. Create Redshift Serverless namespace and workgroup
5. Create Redshift tables
6. Create Glue database, crawlers, and ETL jobs
7. Optionally run the initial data load

## Manual Deployment (Step-by-Step)

If you prefer to run each step individually:

### Step 1: Create S3 Bucket

```bash
./scripts/01_create_s3_bucket.sh
source /tmp/hr_bucket_config.sh
```

### Step 2: Deploy IAM Roles

```bash
aws cloudformation deploy \
    --template-file cloudformation/iam-roles.yaml \
    --stack-name hr-workday-iam-roles \
    --capabilities CAPABILITY_NAMED_IAM \
    --parameter-overrides S3BucketName=${HR_DATA_BUCKET} Environment=dev
```

### Step 3: Upload Data

```bash
./scripts/02_upload_data.sh /path/to/csv/files
```

### Step 4: Create Redshift Serverless

```bash
export REDSHIFT_IAM_ROLE=$(aws cloudformation describe-stacks \
    --stack-name hr-workday-iam-roles \
    --query 'Stacks[0].Outputs[?OutputKey==`RedshiftServerlessRoleArn`].OutputValue' \
    --output text)

./scripts/03_create_redshift_serverless.sh
source /tmp/hr_redshift_config.sh
```

### Step 5: Create Redshift Tables

```bash
./scripts/05_create_redshift_tables.sh
```

### Step 6: Create Glue Resources

```bash
export GLUE_IAM_ROLE=$(aws cloudformation describe-stacks \
    --stack-name hr-workday-iam-roles \
    --query 'Stacks[0].Outputs[?OutputKey==`GlueServiceRoleArn`].OutputValue' \
    --output text)

./scripts/04_create_glue_resources.sh
```

### Step 7: Run Initial Data Load

```bash
aws glue start-job-run --job-name hr-workday-load-to-redshift
```

## Directory Structure

```
aws_deployment/
├── README.md                    # This file
├── deploy.sh                    # Master deployment script
├── cloudformation/
│   └── iam-roles.yaml          # IAM roles for Glue and Redshift
├── scripts/
│   ├── 01_create_s3_bucket.sh  # S3 bucket creation
│   ├── 02_upload_data.sh       # Upload CSV files to S3
│   ├── 03_create_redshift_serverless.sh  # Redshift Serverless setup
│   ├── 04_create_glue_resources.sh       # Glue jobs and crawlers
│   └── 05_create_redshift_tables.sh      # Redshift DDL execution
├── glue_jobs/
│   ├── load_hr_data_copy_command.py      # ETL using COPY (recommended)
│   └── load_hr_data_to_redshift.py       # ETL using Spark
└── sql/
    └── 01_create_schema.sql    # Redshift table definitions
```

## Glue Job Details

### load_hr_data_copy_command.py (Recommended)

Uses Redshift's native COPY command for optimal performance:
- Full-load strategy (truncate and reload)
- Leverages Redshift's parallel data loading
- Best for large datasets

### load_hr_data_to_redshift.py

Alternative Spark-based loader:
- Uses AWS Glue DynamicFrames
- More flexibility for transformations
- Slower for simple loads

## Redshift Schema

All tables are created in the `hr_workday` schema with:
- **DISTKEY**: `employee_id` (optimal for joins)
- **SORTKEY**: `employee_id`, `effective_date` (optimal for time-series queries)
- Audit columns: `loaded_at`, `source_file`

## Scheduling

The Glue job is configured with a daily trigger:
- **Schedule**: Daily at 6:00 AM UTC
- **Trigger Name**: `hr-workday-daily-load`

To modify the schedule:

```bash
aws glue update-trigger \
    --name hr-workday-daily-load \
    --trigger-update "Schedule=cron(0 8 * * ? *)"  # Change to 8 AM
```

## Querying the Data

### Using Redshift Query Editor v2

1. Go to AWS Console → Amazon Redshift → Query Editor v2
2. Connect to workgroup: `hr-workday-wg`
3. Select database: `hr_workday_db`

### Using AWS CLI (Redshift Data API)

```bash
# Count employees
aws redshift-data execute-statement \
    --workgroup-name hr-workday-wg \
    --database hr_workday_db \
    --sql "SELECT COUNT(*) FROM hr_workday.core_hr_employees;"

# Get the results
aws redshift-data get-statement-result --id <statement-id>
```

### Sample Queries

```sql
-- Active employees by business unit
SELECT
    business_unit,
    COUNT(*) as employee_count,
    AVG(base_salary) as avg_salary
FROM hr_workday.core_hr_employees
WHERE worker_status = 'Active'
GROUP BY business_unit
ORDER BY employee_count DESC;

-- Promotions by month
SELECT
    DATE_TRUNC('month', effective_date) as month,
    COUNT(*) as promotions
FROM hr_workday.job_movement_transactions
WHERE transaction_type = 'Promotion'
GROUP BY 1
ORDER BY 1;

-- Compensation changes summary
SELECT
    transaction_type,
    COUNT(*) as count,
    AVG(base_change_percent) as avg_change_pct
FROM hr_workday.compensation_change_transactions
GROUP BY transaction_type;
```

## Cleanup

To remove all resources:

```bash
# Delete Glue resources
aws glue delete-job --job-name hr-workday-load-to-redshift
aws glue delete-trigger --name hr-workday-daily-load
aws glue delete-crawler --name hr-workday-s3-crawler
aws glue delete-database --name hr_workday_catalog

# Delete Redshift Serverless
aws redshift-serverless delete-workgroup --workgroup-name hr-workday-wg
aws redshift-serverless delete-namespace --namespace-name hr-workday-ns

# Delete IAM roles (CloudFormation)
aws cloudformation delete-stack --stack-name hr-workday-iam-roles

# Delete Secrets Manager secret
aws secretsmanager delete-secret --secret-id hr-workday-redshift-credentials --force-delete-without-recovery

# Delete S3 bucket (empty first)
aws s3 rm s3://${HR_DATA_BUCKET} --recursive
aws s3api delete-bucket --bucket ${HR_DATA_BUCKET}
```

## Cost Considerations

| Service | Pricing Model | Estimated Cost |
|---------|--------------|----------------|
| S3 | ~$0.023/GB/month | ~$0.50/month |
| Redshift Serverless | $0.36/RPU-hour | ~$50-100/month (idle) |
| Glue | $0.44/DPU-hour | ~$1/run |

**Tips to reduce costs:**
- Redshift Serverless auto-pauses when idle
- Schedule Glue jobs during off-peak hours
- Use S3 Intelligent-Tiering for infrequent access

## Troubleshooting

### Glue Job Fails

1. Check CloudWatch Logs: `/aws-glue/jobs/error`
2. Verify IAM role permissions
3. Ensure S3 paths are correct

### Redshift Connection Issues

1. Check security group allows inbound on port 5439
2. Verify the workgroup is in "Available" state
3. Check credentials in Secrets Manager

### Data Not Loading

1. Verify CSV files are in the correct S3 path
2. Check COPY command errors in `stl_load_errors`
3. Ensure date formats match (`DATEFORMAT 'auto'`)

## Support

For issues or questions, check:
- AWS Glue documentation
- Redshift Serverless documentation
- CloudFormation troubleshooting guide
